{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing"
      ],
      "metadata": {
        "id": "0ufSGnQcmMUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data import and cleaning\n",
        "\n",
        "# Merged data\n",
        "# Yearly\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import matplotlib.dates as mdates\n",
        "import os\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Goolge colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "fmli_data = {}\n",
        "veq_data = {}\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "    for j in range(1, 5):\n",
        "        if j == 1:\n",
        "            path = f'/content/drive/MyDrive/URAP_Prof Yang/Raw Data/intrvw{i}/intrvw{i}/fmli{i}{j}x.csv'\n",
        "        else:\n",
        "            path = f'/content/drive/MyDrive/URAP_Prof Yang/Raw Data/intrvw{i}/intrvw{i}/fmli{i}{j}.csv'\n",
        "\n",
        "        fmli_data[f'fmli_{i}{j}'] = pd.read_csv(path)\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "    path = f'/content/drive/MyDrive/URAP_Prof Yang/Raw Data/intrvw{i}/expn{i}/veq{i}.csv'\n",
        "    veq_data[f'veq_{i}'] = pd.read_csv(path)\n",
        "\n",
        "\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "    veq_data[f'veq_{i}'] = veq_data[f'veq_{i}'][(veq_data[f'veq_{i}']['QYEAR'] // 10) == (2000 + int(i))] # Convert it into year\n",
        "    veq_data[f'veq_{i}']['CUID'] = veq_data[f'veq_{i}']['NEWID'] // 10 # Consumer units id\n",
        "    veq_data[f'veq_{i}']['QUARTER'] = veq_data[f'veq_{i}']['QYEAR'] % 10 # Gives you each quarter value\n",
        "\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "    for j in range(1, 5):\n",
        "      fmli_data[f'fmli_{i}{j}']['QYEAR'] = f'20{i}{j}'\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "        fmli_data[f'fmli_{i}{j}']['CUID'] = fmli_data[f'fmli_{i}{j}']['NEWID'] // 10\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "    for j in range(1, 5):\n",
        "      fmli_data[f'fmli_{i}{j}']['QYEAR'] = f'20{i}{j}'\n",
        "\n",
        "for i in ['10','11','12','13','14','15','16','17','18','19']:\n",
        "    fmli_data[f'fmli_{i}'] = pd.concat([fmli_data[f'fmli_{i}{1}'], fmli_data[f'fmli_{i}{2}'], fmli_data[f'fmli_{i}{3}'], fmli_data[f'fmli_{i}{4}']], ignore_index=True)\n",
        "\n",
        "for i in ['10','11','12','13','14','15','16','17','18','19']:\n",
        "    fmli_data[f'fmli_{i}']['CUID'] = fmli_data[f'fmli_{i}']['CUID'].astype(int)\n",
        "    fmli_data[f'fmli_{i}']['QYEAR'] = fmli_data[f'fmli_{i}']['QYEAR'].astype(int)\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "  veq_data[f'veq_{i}']['CUID'].astype(int)\n",
        "  veq_data[f'veq_{i}']['QYEAR'].astype(int)\n",
        "\n",
        "merge = {}\n",
        "\n",
        "for i in ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19']:\n",
        "  # Using left-join to merge all of the columns\n",
        "  merge[f'{i}'] = pd.merge(fmli_data[f'fmli_{i}'][['CUID','QYEAR','STATE']],veq_data[f'veq_{i}'][['QYEAR','CUID','VOPSERVY','VOPMOA','VOPEXPX']],on=['CUID', 'QYEAR'], how='left')\n",
        "  # VOPEXPX is the total cost of the maintenance or repair expense\n",
        "  merge[f'{i}']['VOPEXPX'] = pd.to_numeric(merge[f'{i}']['VOPEXPX'], errors='coerce') # fill na\n",
        "  merge[f'{i}']['VOPEXPX'].fillna(0, inplace=True)\n",
        "  # VOPMOA is an indicator variable for the month in which the expense occurred\n",
        "  merge[f'{i}']['VOPMOA'] = pd.to_numeric(merge[f'{i}']['VOPMOA'], errors='coerce') # fill na\n",
        "  merge[f'{i}']['VOPMOA'].fillna(0, inplace=True)\n",
        "  merge[f'{i}']['repair_instances'] = merge[f'{i}']['VOPMOA'].isin(range(1, 13)).astype(int)\n",
        "  merge[f'{i}']['total_repair_expenses'] = merge[f'{i}']['repair_instances'] * merge[f'{i}']['VOPEXPX']\n",
        "\n",
        "final_merge = pd.concat([merge['10'],merge['11'],merge['12'],merge['13'],merge['14'],merge['15'],merge['16'],merge['17'],merge['18'],merge['19']], ignore_index=True)\n",
        "final_merge = final_merge.rename(columns={'QYEAR':'YEAR','VOPEXPX': 'EXPENDITURE','VOPMOA' : \"REPAIR_MONTH\",'total_repair_expenses':'TOTAL_EXPENDITURE','VOPSERVY':'REPAIR_TYPE','repair_instances':'REPAIR_INSTANCES'})\n",
        "final_merge.dropna(subset=[\"STATE\"], inplace=True)\n",
        "\n",
        "# Mapping csv by states\n",
        "fips_to_state = {\n",
        "    1: \"Alabama\",\n",
        "    2: \"Alaska\",\n",
        "    4: \"Arizona\",\n",
        "    5: \"Arkansas\",\n",
        "    6: \"California\",\n",
        "    8: \"Colorado\",\n",
        "    9: \"Connecticut\",\n",
        "    10: \"Delaware\",\n",
        "    11: \"District of Columbia\",\n",
        "    12: \"Florida\",\n",
        "    13: \"Georgia\",\n",
        "    15: \"Hawaii\",\n",
        "    16: \"Idaho\",\n",
        "    17: \"Illinois\",\n",
        "    18: \"Indiana\",\n",
        "    19: \"Iowa\",\n",
        "    20: \"Kansas\",\n",
        "    21: \"Kentucky\",\n",
        "    22: \"Louisiana\",\n",
        "    23: \"Maine\",\n",
        "    24: \"Maryland\",\n",
        "    25: \"Massachusetts\",\n",
        "    26: \"Michigan\",\n",
        "    27: \"Minnesota\",\n",
        "    28: \"Mississippi\",\n",
        "    29: \"Missouri\",\n",
        "    30: \"Montana\",\n",
        "    31: \"Nebraska\",\n",
        "    32: \"Nevada\",\n",
        "    33: \"New Hampshire\",\n",
        "    34: \"New Jersey\",\n",
        "    35: \"New Mexico\",\n",
        "    36: \"New York\",\n",
        "    37: \"North Carolina\",\n",
        "    38: \"North Dakota\",\n",
        "    39: \"Ohio\",\n",
        "    40: \"Oklahoma\",\n",
        "    41: \"Oregon\",\n",
        "    42: \"Pennsylvania\",\n",
        "    44: \"Rhode Island\",\n",
        "    45: \"South Carolina\",\n",
        "    46: \"South Dakota\",\n",
        "    47: \"Tennessee\",\n",
        "    48: \"Texas\",\n",
        "    49: \"Utah\",\n",
        "    50: \"Vermont\",\n",
        "    51: \"Virginia\",\n",
        "    53: \"Washington\",\n",
        "    54: \"West Virginia\",\n",
        "    55: \"Wisconsin\",\n",
        "    56: \"Wyoming\",\n",
        "}\n",
        "\n",
        "final_merge['STATE_NAME'] = final_merge['STATE'].map(fips_to_state)\n",
        "final_merge = final_merge.reindex(columns=['CUID','YEAR','STATE','STATE_NAME','REPAIR_TYPE','REPAIR_MONTH','EXPENDITURE',\t'REPAIR_INSTANCES','TOTAL_EXPENDITURE'])\n",
        "\n",
        "\n",
        "# Repair_type distribution by type and number of repair instances of each household\n",
        "plt.hist(final_merge['REPAIR_TYPE'])\n",
        "# Distribution of repair instances of each household\n",
        "final_merge['REPAIR_INSTANCES'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "9EX5FGDxZmIb",
        "outputId": "9ad764b4-ee3a-4232-a841-68561c93bedc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/URAP_Prof Yang/Raw Data/intrvw10/intrvw10/fmli101x.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a6c0a81b3349>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/drive/MyDrive/URAP_Prof Yang/Raw Data/intrvw{i}/intrvw{i}/fmli{i}{j}.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mfmli_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'fmli_{i}{j}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'11'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'12'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'13'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'14'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'15'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'17'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'18'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'19'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/URAP_Prof Yang/Raw Data/intrvw10/intrvw10/fmli101x.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "Pzh-32jhaalA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign as a dataframe\n",
        "aggregations = {\n",
        "    'YEAR' : 'first',\n",
        "    'CUID' : 'first',\n",
        "    'STATE': 'first',\n",
        "    'REPAIR_INSTANCES': 'sum',\n",
        "    'REPAIR_TYPE' : 'first',\n",
        "    'TOTAL_EXPENDITURE': 'sum'\n",
        "}\n",
        "cleaned_data = final_merge.groupby(['YEAR', 'CUID']).agg(aggregations)\n",
        "cleaned_data['YEAR'] = cleaned_data['YEAR'] // 10"
      ],
      "metadata": {
        "id": "t3iDEuatab8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression with cross-validation"
      ],
      "metadata": {
        "id": "OgKZ9pfScI5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "cleaned_data.dropna(subset = 'REPAIR_TYPE',inplace = True)\n",
        "\n",
        "# tf: Outcome Variable\n",
        "# df: Controls(Types of Repair, Total expenditure)\n",
        "tf = cleaned_data['REPAIR_INSTANCES']\n",
        "df = cleaned_data[['REPAIR_TYPE','TOTAL_EXPENDITURE']]"
      ],
      "metadata": {
        "id": "R28MhvrrcMV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# With and Without intercept logistic regression and hamming loss error\n",
        "\n",
        "df_train, df_test, tf_train, tf_test = train_test_split(df, tf, test_size=0.20)\n",
        "\n",
        "clf_int = LogisticRegression(fit_intercept=True).fit(df_train, tf_train.values.ravel())\n",
        "print('Hamming loss with intercept:', hamming_loss(tf_test, clf_int.predict(df_test)))\n",
        "\n",
        "clf_no_int = LogisticRegression(fit_intercept=False).fit(df_train, tf_train.values.ravel())\n",
        "print('Hamming loss with no intercept:', hamming_loss(tf_test, clf_no_int.predict(df_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og5aQTtseeVM",
        "outputId": "fb007f84-8efc-473c-a454-ceb8c8ef81b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hamming loss with intercept: 0.4858464064998487\n",
            "Hamming loss with no intercept: 0.48601927481740786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "DPYHEu70fWgS",
        "outputId": "0765b266-bc9f-49cf-d934-68d05427778e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "x\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'[ 11570  11571  11572 ... 115690 115691 115692] not in index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-711fb1adc2c8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_submodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_submodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1274\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_level_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2536\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2537\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   2552\u001b[0m                 \u001b[0mcmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2554\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{keyarr[cmask]} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2555\u001b[0m                 \u001b[0;31m# We get here when levels still contain values which are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2556\u001b[0m                 \u001b[0;31m# actually in Index anymore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '[ 11570  11571  11572 ... 115690 115691 115692] not in index'"
          ]
        }
      ]
    }
  ]
}